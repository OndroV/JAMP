)
otu[c(2, 11, 12)]
otu
# HAPLO TYPING STUFF#
# subset haplotypes + writing individual files#
#
dir.create("_data/6_haplotypes") # make folders!#
folder <- paste("_data/6_haplotypes/", sub("_data/5_mapp/(.*)_PE_derep.*", "\\1", blast_names), sep="")#
i <- 1#
for (i in 1:length(blast_names)){#
#
data <- read.csv(blast_names[i], header=F, sep="\t", stringsAsFactors=F)#
#
data$V11 <- sub("(.*);size=.*", "\\1", data$V1)#
data$V12 <- as.numeric(sub(".*;size=(.*);", "\\1", data$V1))#
#
# subset low abundant OTUs#
temp <- aggregate(data$V12, list(data$V2), "sum")#
OTUsum <- sum(temp$x)#
#
temp <- cbind(temp, "relabund"=temp$x/OTUsum*100)#
#
# subset OTUs to keep!#
temp <- temp[temp$relabund>=minOTUabund,]#
#
data <- data[data$V2%in%temp$Group.1,]#
#
report <- paste("Subsetting OTUs with ", minOTUabund, " % anundance; Keeping ", nrow(temp), " OTUs", sep="")#
message(report)#
#
temp_foldername <- folder[i]#
dir.create(temp_foldername)#
#
#Processing of individual OTUs#
# temp = list of OTUs with high enough abundnace (minOTUabund)#
k <- 15#
for(k in 1:nrow(temp)){#
#
dir.create(paste(temp_foldername, "/", temp$Group.1[k], sep=""), showWarnings=F)#
#
meep <- data[data$V2==temp$Group.1[k],]#
#
# convert to rel abundance#
meep$V12 <- meep$V12/sum(meep$V12)*100#
#
meep <- cbind(meep, "keep"=meep$V12 >= AbundInside)#
#
#recalculate rel abundance of OTUs left#
meep <- cbind(meep, "keeprel"=meep$V12)#
meep$keeprel[meep$keep] <- round(meep$keeprel[meep$keep]/sum(meep$keeprel[meep$keep])*100, 2)#
meep$keeprel[!meep$keep] <- NA#
#
# write table as csv!#
write.csv(meep, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_tab.csv", sep=""), row.names=F)#
# save fasta files#
fasta_save <- DNA_master[names(DNA_master)%in%meep$V11[meep$keep]]#
glumanda_names <- meep$V1[meep$keep][match(sub("(.*);size.*", "\\1", meep$V1[meep$keep]), names(fasta_save))] # same sorting#
#
write.fasta(fasta_save, names=glumanda_names, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_sequ.txt", sep=""))#
# make plot!#
pdf(file= paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_plot.pdf", sep=""), width=6, height=6, useDingbats=F)#
plot(meep$V12, ylim=c(0.01, 100), log="y", yaxt="n", ylab="rel. proportions within OTU", main=temp$Group.1[k])#
axis(side=2, at=c(100, 10, 1, 0.1, 0.01, 0.001), labels=c("100", "10", "1", "0.1","0.01", "0.001"), las=2)#
axis(side=2, at=c(seq(20,90,10), seq(2,9,1), seq(0.2,0.9,0.1), seq(0.02,0.09,0.01)), labels=F, las=2, tck=-0.01)#
lines(c(0,nrow(meep)), c(AbundInside, AbundInside), col="Red")#
dev.off()#
#
# make plot, write fasta of sequ to keep#
#
}#
}#
#
i<-1#
# unite haplotypes into single table#
#
master_tab <- data.frame("V11"= "Uniq86")#
#
for (i in 1:length(folder)){#
#
OTUs <- list.files(folder[i])#
#
exp <- NULL # extract individual haplotypes#
for (k in 1:length(OTUs)){#
#
otu <- read.csv(paste(folder[i], "/", OTUs[k], "/", OTUs[k], "_tab.csv", sep=""), stringsAsFactors=F)#
#
# count sequences in OTU#
otu_abund <- sum(as.numeric(sub(".*size=(.*);", "\\1", otu$V1)))#
#
otu <- otu[otu$keep,] # keep high abund haplotypes#
otu <- cbind(otu[c(2, 11, 14)], "indiv"=as.numeric(sub(".*size=(.*);", "\\1", otu$V1)), otu_abund) # extract useful info#
#
exp <- rbind(exp, otu)#
}#
#
temp_name <- sub("_data/6_haplotypes/", "", folder[i])#
names(exp) <- c(paste(temp_name, "_OTU", sep=""), "V11", paste(temp_name, "_rel", sep=""), paste(temp_name, "_abund", sep=""), paste(temp_name, "_OTU_abund", sep=""))#
#
# merge exp haplotype tables from samples#
#
master_tab <- merge(master_tab, exp, by="V11", all=T)#
#
}#
#
# reorganise haplo table #
#
master_tab <- master_tab[c(1, (1:length(folder))*4-2, (1:length(folder))*4-1, (1:length(folder))*4,(1:length(folder))*4+1)]#
#
OTU <- NULL#
for (i in 1:length(folder)){#
keep <- as.vector(!is.na(master_tab[(1+i)]))#
OTU[keep] <- master_tab[keep, (1+i)]#
}#
#
master_tab <- cbind(OTU, master_tab[-c(2:(length(folder)+1))])#
master_tab <- master_tab[order(as.numeric(sub("OTU_", "", master_tab$OTU)), as.numeric(sub("Uniq", "", master_tab$V11)), decreasing=F),]#
write.csv(master_tab, file="haplo_tab.csv")
head(master_tab)
names(master_tab)
names(master_tab)[2]
names(master_tab)[2] <- "Haplotypes"
head(master_tab)
names(DNA_master)
master_tab$Haplotypes
master_tab <- data.frame(OTU, master_tab[-c(2:(length(folder)+1))])
master_tab$Haplotypes
names(master_tab)[2] <- "Haplotypes"
master_tab$Haplotypes
master_tab <- data.frame(OTU, master_tab[-c(2:(length(folder)+1))], stringsAsFactors=F)
names(master_tab)[2] <- "Haplotypes"
master_tab$Haplotypes
otu <- read.csv(paste(folder[i], "/", OTUs[k], "/", OTUs[k], "_tab.csv", sep=""), stringsAsFactors=F)#
#
# count sequences in OTU#
otu_abund <- sum(as.numeric(sub(".*size=(.*);", "\\1", otu$V1)))#
#
otu <- otu[otu$keep,] # keep high abund haplotypes#
otu <- cbind(otu[c(2, 11, 14)], "indiv"=as.numeric(sub(".*size=(.*);", "\\1", otu$V1)), otu_abund) # extract useful info#
#
exp <- rbind(exp, otu)
exp <- NULL # extract individual haplotypes#
for (k in 1:length(OTUs)){#
#
otu <- read.csv(paste(folder[i], "/", OTUs[k], "/", OTUs[k], "_tab.csv", sep=""), stringsAsFactors=F)#
#
# count sequences in OTU#
otu_abund <- sum(as.numeric(sub(".*size=(.*);", "\\1", otu$V1)))#
#
otu <- otu[otu$keep,] # keep high abund haplotypes#
otu <- cbind(otu[c(2, 11, 14)], "indiv"=as.numeric(sub(".*size=(.*);", "\\1", otu$V1)), otu_abund) # extract useful info#
#
exp <- rbind(exp, otu)#
}
str(exp)
i<-1#
# unite haplotypes into single table#
#
master_tab <- data.frame("V11"= "Uniq86")#
#
for (i in 1:length(folder)){#
#
OTUs <- list.files(folder[i])#
#
exp <- NULL # extract individual haplotypes#
for (k in 1:length(OTUs)){#
#
otu <- read.csv(paste(folder[i], "/", OTUs[k], "/", OTUs[k], "_tab.csv", sep=""), stringsAsFactors=F)#
#
# count sequences in OTU#
otu_abund <- sum(as.numeric(sub(".*size=(.*);", "\\1", otu$V1)))#
#
otu <- otu[otu$keep,] # keep high abund haplotypes#
otu <- cbind(otu[c(2, 11, 14)], "indiv"=as.numeric(sub(".*size=(.*);", "\\1", otu$V1)), otu_abund) # extract useful info#
#
exp <- rbind(exp, otu)#
}#
#
temp_name <- sub("_data/6_haplotypes/", "", folder[i])#
names(exp) <- c(paste(temp_name, "_OTU", sep=""), "V11", paste(temp_name, "_rel", sep=""), paste(temp_name, "_abund", sep=""), paste(temp_name, "_OTU_abund", sep=""))#
#
# merge exp haplotype tables from samples#
#
master_tab <- merge(master_tab, exp, by="V11", all=T)#
#
}
str(master_tab)
?merge
master_tab
(master_tab$V11)
as.characters(master_tab$V11)
as.character(master_tab$V11)
master_tab$V11 <- as.character(master_tab$V11)
OTU <- NULL#
#
for (i in 1:length(folder)){#
keep <- as.vector(!is.na(master_tab[(1+i)]))#
OTU[keep] <- master_tab[keep, (1+i)]#
}#
#
master_tab <- cbind(OTU, master_tab[-c(2:(length(folder)+1))])#
master_tab <- master_tab[order(as.numeric(sub("OTU_", "", master_tab$OTU)), as.numeric(sub("Uniq", "", master_tab$V11)), decreasing=F),]#
names(master_tab)[2] <- "Haplotypes"
master_tab$Haplotypes
write.csv(master_tab, file="haplo_tab.csv")
# HAPLO TYPING STUFF#
# subset haplotypes + writing individual files#
#
dir.create("_data/6_haplotypes") # make folders!#
folder <- paste("_data/6_haplotypes/", sub("_data/5_mapp/(.*)_PE_derep.*", "\\1", blast_names), sep="")#
i <- 1#
for (i in 1:length(blast_names)){#
#
data <- read.csv(blast_names[i], header=F, sep="\t", stringsAsFactors=F)#
#
data$V11 <- sub("(.*);size=.*", "\\1", data$V1)#
data$V12 <- as.numeric(sub(".*;size=(.*);", "\\1", data$V1))#
#
# subset low abundant OTUs#
temp <- aggregate(data$V12, list(data$V2), "sum")#
OTUsum <- sum(temp$x)#
#
temp <- cbind(temp, "relabund"=temp$x/OTUsum*100)#
#
# subset OTUs to keep!#
temp <- temp[temp$relabund>=minOTUabund,]#
#
data <- data[data$V2%in%temp$Group.1,]#
#
report <- paste("Subsetting OTUs with ", minOTUabund, " % anundance; Keeping ", nrow(temp), " OTUs", sep="")#
message(report)#
#
temp_foldername <- folder[i]#
dir.create(temp_foldername)#
#
#Processing of individual OTUs#
# temp = list of OTUs with high enough abundnace (minOTUabund)#
k <- 15#
for(k in 1:nrow(temp)){#
#
dir.create(paste(temp_foldername, "/", temp$Group.1[k], sep=""), showWarnings=F)#
#
meep <- data[data$V2==temp$Group.1[k],]#
#
# convert to rel abundance#
meep$V12 <- meep$V12/sum(meep$V12)*100#
#
meep <- cbind(meep, "keep"=meep$V12 >= AbundInside)#
#
#recalculate rel abundance of OTUs left#
meep <- cbind(meep, "keeprel"=meep$V12)#
meep$keeprel[meep$keep] <- round(meep$keeprel[meep$keep]/sum(meep$keeprel[meep$keep])*100, 2)#
meep$keeprel[!meep$keep] <- NA#
#
# write table as csv!#
write.csv(meep, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_tab.csv", sep=""), row.names=F)#
# save fasta files#
fasta_save <- DNA_master[names(DNA_master)%in%meep$V11[meep$keep]]#
glumanda_names <- meep$V1[meep$keep][match(sub("(.*);size.*", "\\1", meep$V1[meep$keep]), names(fasta_save))] # same sorting#
#
write.fasta(fasta_save, names=glumanda_names, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_sequ.txt", sep=""))#
# make plot!#
pdf(file= paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_plot.pdf", sep=""), width=6, height=6, useDingbats=F)#
plot(meep$V12, ylim=c(0.01, 100), log="y", yaxt="n", ylab="rel. proportions within OTU", main=temp$Group.1[k])#
axis(side=2, at=c(100, 10, 1, 0.1, 0.01, 0.001), labels=c("100", "10", "1", "0.1","0.01", "0.001"), las=2)#
axis(side=2, at=c(seq(20,90,10), seq(2,9,1), seq(0.2,0.9,0.1), seq(0.02,0.09,0.01)), labels=F, las=2, tck=-0.01)#
lines(c(0,nrow(meep)), c(AbundInside, AbundInside), col="Red")#
dev.off()#
#
# make plot, write fasta of sequ to keep#
#
}#
}#
#
i<-1#
# unite haplotypes into single table#
#
master_tab <- data.frame("V11"= "Uniq86")#
#
for (i in 1:length(folder)){#
#
OTUs <- list.files(folder[i])#
#
exp <- NULL # extract individual haplotypes#
for (k in 1:length(OTUs)){#
#
otu <- read.csv(paste(folder[i], "/", OTUs[k], "/", OTUs[k], "_tab.csv", sep=""), stringsAsFactors=F)#
#
# count sequences in OTU#
otu_abund <- sum(as.numeric(sub(".*size=(.*);", "\\1", otu$V1)))#
#
otu <- otu[otu$keep,] # keep high abund haplotypes#
otu <- cbind(otu[c(2, 11, 14)], "indiv"=as.numeric(sub(".*size=(.*);", "\\1", otu$V1)), otu_abund) # extract useful info#
#
exp <- rbind(exp, otu)#
}#
#
temp_name <- sub("_data/6_haplotypes/", "", folder[i])#
names(exp) <- c(paste(temp_name, "_OTU", sep=""), "V11", paste(temp_name, "_rel", sep=""), paste(temp_name, "_abund", sep=""), paste(temp_name, "_OTU_abund", sep=""))#
#
# merge exp haplotype tables from samples#
#
master_tab <- merge(master_tab, exp, by="V11", all=T)#
#
}#
#
# reorganise haplo table #
#
master_tab <- master_tab[c(1, (1:length(folder))*4-2, (1:length(folder))*4-1, (1:length(folder))*4,(1:length(folder))*4+1)]#
#
master_tab$V11 <- as.character(master_tab$V11)#
#
OTU <- NULL#
#
for (i in 1:length(folder)){#
keep <- as.vector(!is.na(master_tab[(1+i)]))#
OTU[keep] <- master_tab[keep, (1+i)]#
}#
#
master_tab <- cbind(OTU, master_tab[-c(2:(length(folder)+1))])#
master_tab <- master_tab[order(as.numeric(sub("OTU_", "", master_tab$OTU)), as.numeric(sub("Uniq", "", master_tab$V11)), decreasing=F),]#
names(master_tab)[2] <- "Haplotypes"#
master_tab$Haplotypes#
#
paste(names(DNA_master)[match(sample, DNA_master)], size, sep="")#
#
head(master_tab)
write.csv(master_tab, file="haplo_tab.csv")
master_tab$Haplotypes
match(master_tab$Haplotypes, DNA_master)
match(master_tab$Haplotypes, names(DNA_master))
DNA_master[match(master_tab$Haplotypes, names(DNA_master))]
master_tab$Haplotypes
cbind(master_tab, "sequ"=DNA_master[match(master_tab$Haplotypes, names(DNA_master))])
DNA_master[match(master_tab$Haplotypes, names(DNA_master))]
unlist(DNA_master[match(master_tab$Haplotypes, names(DNA_master))])
cbind(master_tab, "sequ"=unlist(DNA_master[match(master_tab$Haplotypes, names(DNA_master))]))
master_tab <- cbind(sort= c(1:nrow(master_tab)), master_tab, "sequ"=unlist(DNA_master[match(master_tab$Haplotypes, names(DNA_master))]))
write.csv(master_tab, file="haplo_tab.csv")
write.csv(master_tab, file="haplo_tab.csv", row.names=F)
# HAPLO TYPING STUFF#
# subset haplotypes + writing individual files#
#
dir.create("_data/6_haplotypes") # make folders!#
folder <- paste("_data/6_haplotypes/", sub("_data/5_mapp/(.*)_PE_derep.*", "\\1", blast_names), sep="")#
i <- 1#
for (i in 1:length(blast_names)){#
#
data <- read.csv(blast_names[i], header=F, sep="\t", stringsAsFactors=F)#
#
data$V11 <- sub("(.*);size=.*", "\\1", data$V1)#
data$V12 <- as.numeric(sub(".*;size=(.*);", "\\1", data$V1))#
#
# subset low abundant OTUs#
temp <- aggregate(data$V12, list(data$V2), "sum")#
OTUsum <- sum(temp$x)#
#
temp <- cbind(temp, "relabund"=temp$x/OTUsum*100)#
#
# subset OTUs to keep!#
temp <- temp[temp$relabund>=minOTUabund,]#
#
data <- data[data$V2%in%temp$Group.1,]#
#
report <- paste("Subsetting OTUs with ", minOTUabund, " % anundance; Keeping ", nrow(temp), " OTUs", sep="")#
message(report)#
#
temp_foldername <- folder[i]#
dir.create(temp_foldername)#
#
#Processing of individual OTUs#
# temp = list of OTUs with high enough abundnace (minOTUabund)#
k <- 15#
for(k in 1:nrow(temp)){#
#
dir.create(paste(temp_foldername, "/", temp$Group.1[k], sep=""), showWarnings=F)#
#
meep <- data[data$V2==temp$Group.1[k],]#
#
# convert to rel abundance#
meep$V12 <- meep$V12/sum(meep$V12)*100#
#
meep <- cbind(meep, "keep"=meep$V12 >= AbundInside)#
#
#recalculate rel abundance of OTUs left#
meep <- cbind(meep, "keeprel"=meep$V12)#
meep$keeprel[meep$keep] <- round(meep$keeprel[meep$keep]/sum(meep$keeprel[meep$keep])*100, 2)#
meep$keeprel[!meep$keep] <- NA#
#
# write table as csv!#
write.csv(meep, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_tab.csv", sep=""), row.names=F)#
# save fasta files#
fasta_save <- DNA_master[names(DNA_master)%in%meep$V11[meep$keep]]#
glumanda_names <- meep$V1[meep$keep][match(sub("(.*);size.*", "\\1", meep$V1[meep$keep]), names(fasta_save))] # same sorting#
#
write.fasta(fasta_save, names=glumanda_names, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_sequ.txt", sep=""))#
# make plot!#
pdf(file= paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_plot.pdf", sep=""), width=6, height=6, useDingbats=F)#
plot(meep$V12, ylim=c(0.01, 100), log="y", yaxt="n", ylab="rel. proportions within OTU", main=temp$Group.1[k])#
axis(side=2, at=c(100, 10, 1, 0.1, 0.01, 0.001), labels=c("100", "10", "1", "0.1","0.01", "0.001"), las=2)#
axis(side=2, at=c(seq(20,90,10), seq(2,9,1), seq(0.2,0.9,0.1), seq(0.02,0.09,0.01)), labels=F, las=2, tck=-0.01)#
lines(c(0,nrow(meep)), c(AbundInside, AbundInside), col="Red")#
dev.off()#
#
# make plot, write fasta of sequ to keep#
#
}#
}#
#
i<-1#
# unite haplotypes into single table#
#
master_tab <- data.frame("V11"= "Uniq86")#
#
for (i in 1:length(folder)){#
#
OTUs <- list.files(folder[i])#
#
exp <- NULL # extract individual haplotypes#
for (k in 1:length(OTUs)){#
#
otu <- read.csv(paste(folder[i], "/", OTUs[k], "/", OTUs[k], "_tab.csv", sep=""), stringsAsFactors=F)#
#
# count sequences in OTU#
otu_abund <- sum(as.numeric(sub(".*size=(.*);", "\\1", otu$V1)))#
#
otu <- otu[otu$keep,] # keep high abund haplotypes#
otu <- cbind(otu[c(2, 11, 14)], "indiv"=as.numeric(sub(".*size=(.*);", "\\1", otu$V1)), otu_abund) # extract useful info#
#
exp <- rbind(exp, otu)#
}#
#
temp_name <- sub("_data/6_haplotypes/", "", folder[i])#
names(exp) <- c(paste(temp_name, "_OTU", sep=""), "V11", paste(temp_name, "_rel", sep=""), paste(temp_name, "_abund", sep=""), paste(temp_name, "_OTU_abund", sep=""))#
#
# merge exp haplotype tables from samples#
#
master_tab <- merge(master_tab, exp, by="V11", all=T)#
#
}#
#
# reorganise haplo table #
#
master_tab <- master_tab[c(1, (1:length(folder))*4-2, (1:length(folder))*4-1, (1:length(folder))*4,(1:length(folder))*4+1)]#
#
master_tab$V11 <- as.character(master_tab$V11)#
#
OTU <- NULL#
#
for (i in 1:length(folder)){#
keep <- as.vector(!is.na(master_tab[(1+i)]))#
OTU[keep] <- master_tab[keep, (1+i)]#
}#
#
master_tab <- cbind(OTU, master_tab[-c(2:(length(folder)+1))])#
master_tab <- master_tab[order(as.numeric(sub("OTU_", "", master_tab$OTU)), as.numeric(sub("Uniq", "", master_tab$V11)), decreasing=F),]#
names(master_tab)[2] <- "Haplotypes"#
#
# add sequences to table#
master_tab <- cbind(sort= c(1:nrow(master_tab)), master_tab, "sequ"=unlist(DNA_master[match(master_tab$Haplotypes, names(DNA_master))]))#
#
write.csv(master_tab, file="haplo_tab.csv", row.names=F)
DNA_master[match(master_tab$Haplotypes, names(DNA_master))])
DNA_master[match(master_tab$Haplotypes, names(DNA_master))]
master_tab
head(master_tab)
write.fasta(DNA_master[match(master_tab$Haplotypes, names(DNA_master))], names=paste(master_tab$OTU, master_tab$Haplotypes, sep=""), "haplo_fasta.txt")
write.fasta(DNA_master[match(master_tab$Haplotypes, names(DNA_master))], names=paste(master_tab$OTU, master_tab$Haplotypes, sep="_"), "haplo_fasta.txt")
# Haplotyping v0.1#
#
haplotyping <- function(files="latest", ampliconLength=NA, minsize=5, minrelsize=0.001, minOTUabund=0.1, AbundInside=1, otu_radius_pct=3, strand="plus"){#
Core(module="Haplotyping")#
cat(file="../log.txt", c("Version v0.1", "\n"), append=T, sep="\n")#
message(" ")#
#
if (files[1]=="latest"){#
source("robots.txt")#
files <- list.files(paste("../", last_data, "/_data", sep=""), full.names=T)#
}#
#
# Dereplicate files using Usearch#
dir.create("_data/1_derep")#
#
# count sequences in each file#
counts <- Count_sequences(files, fastq=F)#
size <- round(counts* minrelsize/100) # get nim abundance#
size[size<minsize] <- minrelsize # min size!#
new_names <- sub(".*(_data/.*)", "\\1", files)#
new_names <- sub("_PE.*", "_PE_derep", new_names)#
new_names <- paste(new_names, "_", size, ".txt", sep="")#
new_names <- sub("_data", "_data/1_derep", new_names)#
#
cmd <- paste("-fastx_uniques \"", files, "\" -fastaout \"", new_names, "\" -sizeout", " -minuniquesize ", size,  sep="")#
#
temp <- paste(length(files), " files are dereplicated and sequences bwelow ", minrelsize, "% (or minuniqesize of ", minsize, ") are beeing discarded:", sep="")#
cat(file="../log.txt", temp , append=T, sep="\n")#
message(temp)#
#
temp <- new_names#
for (i in 1:length(cmd)){#
A <- system2("usearch", cmd[i], stdout=T, stderr=T)#
meep <- sub(".*_data/(.*)", "\\1", temp[i])#
cat(file="_stats/1_derep_logs.txt", meep, A, "\n", append=T, sep="\n")#
#
log_count <- Count_sequences(paste("_data/", meep, sep=""))#
log <- paste(sub(".*_data/1_derep/(.*)", "\\1", temp[i]), ": ", log_count, " of ", counts[i], " keept (", round((log_count/counts[i])*100, digits=4), "%, min size: ", size[i],")", sep="")#
cat(file="../log.txt", log , append=T, sep="\n")#
message(log)#
}#
#
dir.create("_data/2_MinMax")#
#
new_names2 <- sub("1_derep", "2_MinMax", new_names)#
# min max sequence length (cutadapt), All files!#
temp <- paste("Filtering ", length(new_names), " files for Min/Max length, keeping only sequences that are ", ampliconLength, " bp long!", sep="")#
message(temp)#
cat(file="../log.txt", temp , append=T, sep="\n")#
#
cmd <- paste(new_names, " -o ", new_names2, " -f \"fasta\" -m ", ampliconLength, " -M ", ampliconLength, sep="")#
for (i in 1:length(new_names2)){#
A <- system2("cutadapt", cmd[i], stdout=T, stderr=T)#
getwd()#
#
cat(file="_stats/2_MinMax.txt", "\n", A, "", paste("cutadapt", cmd[i]), append=T, sep="\n")#
#
stats <- A#
reads_in <- stats[grep("Total reads processed:", stats)[1]]#
reads_in <- sub(".* processed: +", "", reads_in)#
reads_in <- as.numeric(gsub(",", "", reads_in))#
#
reads_out <- stats[grep("Reads written \\(passing filters\\):", stats)[1]]#
reads_out <- sub(".* filters.: +", "", reads_out)#
reads_out <- sub(" .*", "", reads_out)#
reads_out <- as.numeric(gsub(",", "", reads_out))#
#
keep <- round(reads_out/reads_in*100, digits=2)#
meep <- paste("Filtering ", reads_in, " reads with min max ", ampliconLength, " bp: keep ", reads_out, " (", keep, "%)", sep="")#
cat(file="../log.txt", meep, append=T, sep="\n")#
message(meep)#
}#
# 2 make OTUs!#
# merge all files into one#
#
dir.create("_data/3_OTU_clustering")#
#
cmd <- paste(paste(new_names2, collapse=" "), "> _data/3_OTU_clustering/A_all_files_united.fasta", collapse=" ")#
A <- system2("cat", cmd, stdout=T, stderr=T)#
#
temp <- paste(length(files), " dereplicated files where merged into file:\n\"_data/3_OTU_clustering/A_all_files_united.fasta\"", sep="")#
message("\n", temp)#
cat(file="../log.txt", "\n", temp, append=T, sep="\n")#
cat(file="_stats/3_OTU_clustering_log.txt", temp, "", paste("cat", cmd), append=T, sep="\n")#
#
# dereplicate "A_all_files_united.fasta" using Vsearch!#
cmd <- paste("-derep_fulllength _data/3_OTU_clustering/A_all_files_united.fasta -output _data/3_OTU_clustering/B_all_derep.fasta -sizein -sizeout -relabel Uniq", sep="")#
#
A <- system2("vsearch", cmd, stdout=T, stderr=T)#
#
temp <- paste("Total number of sequences (not dereplicated): ", sub(".*nt in (.*) seqs.*", "\\1", A[grep("seqs, min", A)]), "\n", sep="")#
message(temp)#
cat(file="../log.txt", temp, append=T, sep="\n")#
#
temp <- paste("United sequences are dereplicated + size filtered into a total of ", sub("(.*) unique sequences.*", "\\1", A[grep(" unique sequences", A)]), " unique sequences.", "\n", "File prepared for OTU clustering: B_all_derep.fasta", sep="")#
message(temp)#
cat(file="../log.txt", temp, append=T, sep="\n")#
#
# derep log#
cat(file="_stats/3_OTU_clustering_log.txt", "\n", A, "", paste("cat", cmd), append=T, sep="\n")#
# Actual OTU clustering of dereplicated filtered file! #
#
cmd <- paste(" -cluster_otus _data/3_OTU_clustering/B_all_derep.fasta -otus _data/3_OTU_clustering/C_OTUs.fasta -uparseout _data/3_OTU_clustering/C_OTU_table.txt -relabel OTU_ -otu_radius_pct ", otu_radius_pct, " -strand ", strand, sep="")#
#
A <- system2("usearch", cmd, stdout=T, stderr=T) # cluster OTUs!#
#
# cluster log#
cat(file="_stats/3_OTU_clustering_log.txt", "\n", paste("usearch", cmd), "", A, "", append=T, sep="\n")#
#
chimeras <- sub(".*OTUs, (.*) chimeras\r", "\\1", A[grep("chimeras\r", A)])#
OTUs <- sub(".*100.0% (.*) OTUs, .* chimeras\r", "\\1", A[grep("chimeras\r", A)])#
#
temp <- paste("\n", "Clustering reads from\n\"B_all_derep.fasta\" \notu_radius_pct = ", otu_radius_pct, "\nstrand = ", strand, "\nChimeras discarded: ", chimeras, "\nOTUs written: ", OTUs, " -> file \"C_OTUs.fasta\"\n", sep="")#
message(temp)#
cat(file="../log.txt", temp, append=T, sep="\n")#
#
# RNENAME#
# compare reads against dereplicated and RENAME sequences!#
#
dir.create("_data/4_rename")#
#
DNA_master <- read.fasta("_data/3_OTU_clustering/B_all_derep.fasta", as.string=T, forceDNAtolower=F)#
names(DNA_master) <- sub("(.*);size=.*", "\\1", names(DNA_master))#
#
new_names3 <- sub("2_MinMax", "4_rename", new_names2)#
#
for (i in 1:length(new_names2)){#
#
sample <- read.fasta(new_names2[i], as.string=T, forceDNAtolower=F)#
#
size <- sub(".*(;size=.*;)", "\\1", names(sample))#
sequnames <- paste(names(DNA_master)[match(sample, DNA_master)], size, sep="")#
#
write.fasta(sample, names= sequnames, new_names3[i])#
}#
message("read renamed! ame as in \"B_all_derep.fasta\"")#
# END renaming#
# Mapp reads (filtered abund & MM) against OTUs#
blast_names <- sub("4_rename", "5_mapp", new_names3)#
log_names <- sub("_data", "_stats", blast_names)#
dir.create("_data/5_mapp")#
#
cmd <- paste("-usearch_global ", new_names3, " -db ", "\"_data/3_OTU_clustering/C_OTUs.fasta\"", " -strand plus -id ", (100-otu_radius_pct)/100, " -blast6out \"", blast_names, "\" -maxhits 1", sep="")#
for (i in 1:length(cmd)){#
A <- system2("usearch", cmd[i], stdout=T, stderr=T)#
cat(file= "_stats/5_mapping.txt", paste("vsearch", cmd[i], sep=""), A, "\n\n\n", append=T, sep="\n")#
}#
message("Reads remapped!")#
# HAPLO TYPING STUFF#
# subset haplotypes + writing individual files#
#
dir.create("_data/6_haplotypes") # make folders!#
folder <- paste("_data/6_haplotypes/", sub("_data/5_mapp/(.*)_PE_derep.*", "\\1", blast_names), sep="")#
i <- 1#
for (i in 1:length(blast_names)){#
#
data <- read.csv(blast_names[i], header=F, sep="\t", stringsAsFactors=F)#
#
data$V11 <- sub("(.*);size=.*", "\\1", data$V1)#
data$V12 <- as.numeric(sub(".*;size=(.*);", "\\1", data$V1))#
#
# subset low abundant OTUs#
temp <- aggregate(data$V12, list(data$V2), "sum")#
OTUsum <- sum(temp$x)#
#
temp <- cbind(temp, "relabund"=temp$x/OTUsum*100)#
#
# subset OTUs to keep!#
temp <- temp[temp$relabund>=minOTUabund,]#
#
data <- data[data$V2%in%temp$Group.1,]#
#
report <- paste("Subsetting OTUs with ", minOTUabund, " % anundance; Keeping ", nrow(temp), " OTUs", sep="")#
message(report)#
#
temp_foldername <- folder[i]#
dir.create(temp_foldername)#
#
#Processing of individual OTUs#
# temp = list of OTUs with high enough abundnace (minOTUabund)#
k <- 15#
for(k in 1:nrow(temp)){#
#
dir.create(paste(temp_foldername, "/", temp$Group.1[k], sep=""), showWarnings=F)#
#
meep <- data[data$V2==temp$Group.1[k],]#
#
# convert to rel abundance#
meep$V12 <- meep$V12/sum(meep$V12)*100#
#
meep <- cbind(meep, "keep"=meep$V12 >= AbundInside)#
#
#recalculate rel abundance of OTUs left#
meep <- cbind(meep, "keeprel"=meep$V12)#
meep$keeprel[meep$keep] <- round(meep$keeprel[meep$keep]/sum(meep$keeprel[meep$keep])*100, 2)#
meep$keeprel[!meep$keep] <- NA#
#
# write table as csv!#
write.csv(meep, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_tab.csv", sep=""), row.names=F)#
# save fasta files#
fasta_save <- DNA_master[names(DNA_master)%in%meep$V11[meep$keep]]#
glumanda_names <- meep$V1[meep$keep][match(sub("(.*);size.*", "\\1", meep$V1[meep$keep]), names(fasta_save))] # same sorting#
#
write.fasta(fasta_save, names=glumanda_names, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_sequ.txt", sep=""))#
# make plot!#
pdf(file= paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_plot.pdf", sep=""), width=6, height=6, useDingbats=F)#
plot(meep$V12, ylim=c(0.01, 100), log="y", yaxt="n", ylab="rel. proportions within OTU", main=temp$Group.1[k])#
axis(side=2, at=c(100, 10, 1, 0.1, 0.01, 0.001), labels=c("100", "10", "1", "0.1","0.01", "0.001"), las=2)#
axis(side=2, at=c(seq(20,90,10), seq(2,9,1), seq(0.2,0.9,0.1), seq(0.02,0.09,0.01)), labels=F, las=2, tck=-0.01)#
lines(c(0,nrow(meep)), c(AbundInside, AbundInside), col="Red")#
dev.off()#
#
# make plot, write fasta of sequ to keep#
#
}#
}#
#
i<-1#
# unite haplotypes into single table#
#
master_tab <- data.frame("V11"= "Uniq86")#
#
for (i in 1:length(folder)){#
#
OTUs <- list.files(folder[i])#
#
exp <- NULL # extract individual haplotypes#
for (k in 1:length(OTUs)){#
#
otu <- read.csv(paste(folder[i], "/", OTUs[k], "/", OTUs[k], "_tab.csv", sep=""), stringsAsFactors=F)#
#
# count sequences in OTU#
otu_abund <- sum(as.numeric(sub(".*size=(.*);", "\\1", otu$V1)))#
#
otu <- otu[otu$keep,] # keep high abund haplotypes#
otu <- cbind(otu[c(2, 11, 14)], "indiv"=as.numeric(sub(".*size=(.*);", "\\1", otu$V1)), otu_abund) # extract useful info#
#
exp <- rbind(exp, otu)#
}#
#
temp_name <- sub("_data/6_haplotypes/", "", folder[i])#
names(exp) <- c(paste(temp_name, "_OTU", sep=""), "V11", paste(temp_name, "_rel", sep=""), paste(temp_name, "_abund", sep=""), paste(temp_name, "_OTU_abund", sep=""))#
#
# merge exp haplotype tables from samples#
#
master_tab <- merge(master_tab, exp, by="V11", all=T)#
#
}#
#
# reorganise haplo table #
#
master_tab <- master_tab[c(1, (1:length(folder))*4-2, (1:length(folder))*4-1, (1:length(folder))*4,(1:length(folder))*4+1)]#
#
master_tab$V11 <- as.character(master_tab$V11)#
#
OTU <- NULL#
#
for (i in 1:length(folder)){#
keep <- as.vector(!is.na(master_tab[(1+i)]))#
OTU[keep] <- master_tab[keep, (1+i)]#
}#
#
master_tab <- cbind(OTU, master_tab[-c(2:(length(folder)+1))])#
master_tab <- master_tab[order(as.numeric(sub("OTU_", "", master_tab$OTU)), as.numeric(sub("Uniq", "", master_tab$V11)), decreasing=F),]#
names(master_tab)[2] <- "Haplotypes"#
#
# add sequences to table#
master_tab <- cbind(sort= c(1:nrow(master_tab)), master_tab, "sequ"=unlist(DNA_master[match(master_tab$Haplotypes, names(DNA_master))]))#
#
write.csv(master_tab, file="haplo_tab.csv", row.names=F)#
write.fasta(DNA_master[match(master_tab$Haplotypes, names(DNA_master))], names=paste(master_tab$OTU, master_tab$Haplotypes, sep="_"), "haplo_fasta.txt")#
temp <- "\nModule completed!"#
message(temp)#
cat(file="../log.txt", paste(Sys.time(), temp, "", sep="\n"), append=T, sep="\n")#
#
setwd("../")#
}
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
temp <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[11:16]#
haplotyping(temp, 178)
temp <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[11:16]
temp
temp <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[25:30]
temp
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[1:8]
files
temp <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[11:16]
temp
temp <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[25:30]
temp
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
temp <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[25:30]#
haplotyping(temp, 178)
# Haplotyping v0.1#
#
haplotyping <- function(files="latest", ampliconLength=NA, minsize=5, minrelsize=0.001, minOTUabund=0.1, AbundInside=1, otu_radius_pct=3, strand="plus"){#
Core(module="Haplotyping")#
cat(file="../log.txt", c("Version v0.1", "\n"), append=T, sep="\n")#
message(" ")#
#
if (files[1]=="latest"){#
source("robots.txt")#
files <- list.files(paste("../", last_data, "/_data", sep=""), full.names=T)#
}#
#
# Dereplicate files using Usearch#
dir.create("_data/1_derep")#
#
# count sequences in each file#
counts <- Count_sequences(files, fastq=F)#
size <- round(counts* minrelsize/100) # get nim abundance#
size[size<minsize] <- minrelsize # min size!#
new_names <- sub(".*(_data/.*)", "\\1", files)#
new_names <- sub("_PE.*", "_PE_derep", new_names)#
new_names <- paste(new_names, "_", size, ".txt", sep="")#
new_names <- sub("_data", "_data/1_derep", new_names)#
#
cmd <- paste("-fastx_uniques \"", files, "\" -fastaout \"", new_names, "\" -sizeout", " -minuniquesize ", size,  sep="")#
#
temp <- paste(length(files), " files are dereplicated and sequences bwelow ", minrelsize, "% (or minuniqesize of ", minsize, ") are beeing discarded:", sep="")#
cat(file="../log.txt", temp , append=T, sep="\n")#
message(temp)#
#
temp <- new_names#
for (i in 1:length(cmd)){#
A <- system2("usearch", cmd[i], stdout=T, stderr=T)#
meep <- sub(".*_data/(.*)", "\\1", temp[i])#
cat(file="_stats/1_derep_logs.txt", meep, A, "\n", append=T, sep="\n")#
#
log_count <- Count_sequences(paste("_data/", meep, sep=""))#
log <- paste(sub(".*_data/1_derep/(.*)", "\\1", temp[i]), ": ", log_count, " of ", counts[i], " keept (", round((log_count/counts[i])*100, digits=4), "%, min size: ", size[i],")", sep="")#
cat(file="../log.txt", log , append=T, sep="\n")#
message(log)#
}#
#
dir.create("_data/2_MinMax")#
#
new_names2 <- sub("1_derep", "2_MinMax", new_names)#
# min max sequence length (cutadapt), All files!#
temp <- paste("Filtering ", length(new_names), " files for Min/Max length, keeping only sequences that are ", ampliconLength, " bp long!", sep="")#
message(temp)#
cat(file="../log.txt", temp , append=T, sep="\n")#
#
cmd <- paste(new_names, " -o ", new_names2, " -f \"fasta\" -m ", ampliconLength, " -M ", ampliconLength, sep="")#
for (i in 1:length(new_names2)){#
A <- system2("cutadapt", cmd[i], stdout=T, stderr=T)#
getwd()#
#
cat(file="_stats/2_MinMax.txt", "\n", A, "", paste("cutadapt", cmd[i]), append=T, sep="\n")#
#
stats <- A#
reads_in <- stats[grep("Total reads processed:", stats)[1]]#
reads_in <- sub(".* processed: +", "", reads_in)#
reads_in <- as.numeric(gsub(",", "", reads_in))#
#
reads_out <- stats[grep("Reads written \\(passing filters\\):", stats)[1]]#
reads_out <- sub(".* filters.: +", "", reads_out)#
reads_out <- sub(" .*", "", reads_out)#
reads_out <- as.numeric(gsub(",", "", reads_out))#
#
keep <- round(reads_out/reads_in*100, digits=2)#
meep <- paste("Filtering ", reads_in, " reads with min max ", ampliconLength, " bp: keep ", reads_out, " (", keep, "%)", sep="")#
cat(file="../log.txt", meep, append=T, sep="\n")#
message(meep)#
}#
# 2 make OTUs!#
# merge all files into one#
#
dir.create("_data/3_OTU_clustering")#
#
cmd <- paste(paste(new_names2, collapse=" "), "> _data/3_OTU_clustering/A_all_files_united.fasta", collapse=" ")#
A <- system2("cat", cmd, stdout=T, stderr=T)#
#
temp <- paste(length(files), " dereplicated files where merged into file:\n\"_data/3_OTU_clustering/A_all_files_united.fasta\"", sep="")#
message("\n", temp)#
cat(file="../log.txt", "\n", temp, append=T, sep="\n")#
cat(file="_stats/3_OTU_clustering_log.txt", temp, "", paste("cat", cmd), append=T, sep="\n")#
#
# dereplicate "A_all_files_united.fasta" using Vsearch!#
cmd <- paste("-derep_fulllength _data/3_OTU_clustering/A_all_files_united.fasta -output _data/3_OTU_clustering/B_all_derep.fasta -sizein -sizeout -relabel Uniq", sep="")#
#
A <- system2("vsearch", cmd, stdout=T, stderr=T)#
#
temp <- paste("Total number of sequences (not dereplicated): ", sub(".*nt in (.*) seqs.*", "\\1", A[grep("seqs, min", A)]), "\n", sep="")#
message(temp)#
cat(file="../log.txt", temp, append=T, sep="\n")#
#
temp <- paste("United sequences are dereplicated + size filtered into a total of ", sub("(.*) unique sequences.*", "\\1", A[grep(" unique sequences", A)]), " unique sequences.", "\n", "File prepared for OTU clustering: B_all_derep.fasta", sep="")#
message(temp)#
cat(file="../log.txt", temp, append=T, sep="\n")#
#
# derep log#
cat(file="_stats/3_OTU_clustering_log.txt", "\n", A, "", paste("cat", cmd), append=T, sep="\n")#
# Actual OTU clustering of dereplicated filtered file! #
#
cmd <- paste(" -cluster_otus _data/3_OTU_clustering/B_all_derep.fasta -otus _data/3_OTU_clustering/C_OTUs.fasta -uparseout _data/3_OTU_clustering/C_OTU_table.txt -relabel OTU_ -otu_radius_pct ", otu_radius_pct, " -strand ", strand, sep="")#
#
A <- system2("usearch", cmd, stdout=T, stderr=T) # cluster OTUs!#
#
# cluster log#
cat(file="_stats/3_OTU_clustering_log.txt", "\n", paste("usearch", cmd), "", A, "", append=T, sep="\n")#
#
chimeras <- sub(".*OTUs, (.*) chimeras\r", "\\1", A[grep("chimeras\r", A)])#
OTUs <- sub(".*100.0% (.*) OTUs, .* chimeras\r", "\\1", A[grep("chimeras\r", A)])#
#
temp <- paste("\n", "Clustering reads from\n\"B_all_derep.fasta\" \notu_radius_pct = ", otu_radius_pct, "\nstrand = ", strand, "\nChimeras discarded: ", chimeras, "\nOTUs written: ", OTUs, " -> file \"C_OTUs.fasta\"\n", sep="")#
message(temp)#
cat(file="../log.txt", temp, append=T, sep="\n")#
#
# RNENAME#
# compare reads against dereplicated and RENAME sequences!#
#
dir.create("_data/4_rename")#
#
DNA_master <- read.fasta("_data/3_OTU_clustering/B_all_derep.fasta", as.string=T, forceDNAtolower=F)#
names(DNA_master) <- sub("(.*);size=.*", "\\1", names(DNA_master))#
#
new_names3 <- sub("2_MinMax", "4_rename", new_names2)#
#
for (i in 1:length(new_names2)){#
#
sample <- read.fasta(new_names2[i], as.string=T, forceDNAtolower=F)#
#
size <- sub(".*(;size=.*;)", "\\1", names(sample))#
sequnames <- paste(names(DNA_master)[match(sample, DNA_master)], size, sep="")#
#
write.fasta(sample, names= sequnames, new_names3[i])#
}#
message("read renamed! ame as in \"B_all_derep.fasta\"")#
# END renaming#
# Mapp reads (filtered abund & MM) against OTUs#
blast_names <- sub("4_rename", "5_mapp", new_names3)#
log_names <- sub("_data", "_stats", blast_names)#
dir.create("_data/5_mapp")#
#
cmd <- paste("-usearch_global ", new_names3, " -db ", "\"_data/3_OTU_clustering/C_OTUs.fasta\"", " -strand plus -id ", (100-otu_radius_pct)/100, " -blast6out \"", blast_names, "\" -maxhits 1", sep="")#
for (i in 1:length(cmd)){#
A <- system2("usearch", cmd[i], stdout=T, stderr=T)#
cat(file= "_stats/5_mapping.txt", paste("vsearch", cmd[i], sep=""), A, "\n\n\n", append=T, sep="\n")#
}#
message("Reads remapped!")#
# HAPLO TYPING STUFF#
# subset haplotypes + writing individual files#
#
dir.create("_data/6_haplotypes") # make folders!#
folder <- paste("_data/6_haplotypes/", sub("_data/5_mapp/(.*)_PE_derep.*", "\\1", blast_names), sep="")#
i <- 1#
for (i in 1:length(blast_names)){#
#
data <- read.csv(blast_names[i], header=F, sep="\t", stringsAsFactors=F)#
#
data$V11 <- sub("(.*);size=.*", "\\1", data$V1)#
data$V12 <- as.numeric(sub(".*;size=(.*);", "\\1", data$V1))#
#
# subset low abundant OTUs#
temp <- aggregate(data$V12, list(data$V2), "sum")#
OTUsum <- sum(temp$x)#
#
temp <- cbind(temp, "relabund"=temp$x/OTUsum*100)#
#
# subset OTUs to keep!#
temp <- temp[temp$relabund>=minOTUabund,]#
#
data <- data[data$V2%in%temp$Group.1,]#
#
report <- paste("Subsetting OTUs with ", minOTUabund, " % anundance; Keeping ", nrow(temp), " OTUs", sep="")#
message(report)#
#
temp_foldername <- folder[i]#
dir.create(temp_foldername)#
#
#Processing of individual OTUs#
# temp = list of OTUs with high enough abundnace (minOTUabund)#
k <- 15#
for(k in 1:nrow(temp)){#
#
dir.create(paste(temp_foldername, "/", temp$Group.1[k], sep=""), showWarnings=F)#
#
meep <- data[data$V2==temp$Group.1[k],]#
#
# convert to rel abundance#
meep$V12 <- meep$V12/sum(meep$V12)*100#
#
meep <- cbind(meep, "keep"=meep$V12 >= AbundInside)#
#
#recalculate rel abundance of OTUs left#
meep <- cbind(meep, "keeprel"=meep$V12)#
meep$keeprel[meep$keep] <- round(meep$keeprel[meep$keep]/sum(meep$keeprel[meep$keep])*100, 2)#
meep$keeprel[!meep$keep] <- NA#
#
# write table as csv!#
write.csv(meep, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_tab.csv", sep=""), row.names=F)#
# save fasta files#
fasta_save <- DNA_master[names(DNA_master)%in%meep$V11[meep$keep]]#
glumanda_names <- meep$V1[meep$keep][match(sub("(.*);size.*", "\\1", meep$V1[meep$keep]), names(fasta_save))] # same sorting#
#
write.fasta(fasta_save, names=glumanda_names, paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_sequ.txt", sep=""))#
# make plot!#
pdf(file= paste(temp_foldername, "/", temp$Group.1[k], "/", temp$Group.1[k], "_plot.pdf", sep=""), width=6, height=6, useDingbats=F)#
plot(meep$V12, ylim=c(0.01, 100), log="y", yaxt="n", ylab="rel. proportions within OTU", main=temp$Group.1[k])#
axis(side=2, at=c(100, 10, 1, 0.1, 0.01, 0.001), labels=c("100", "10", "1", "0.1","0.01", "0.001"), las=2)#
axis(side=2, at=c(seq(20,90,10), seq(2,9,1), seq(0.2,0.9,0.1), seq(0.02,0.09,0.01)), labels=F, las=2, tck=-0.01)#
lines(c(0,nrow(meep)), c(AbundInside, AbundInside), col="Red")#
dev.off()#
#
# make plot, write fasta of sequ to keep#
#
}#
}#
#
i<-1#
# unite haplotypes into single table#
#
master_tab <- data.frame("V11"= "Uniq86")#
#
for (i in 1:length(folder)){#
#
OTUs <- list.files(folder[i])#
#
exp <- NULL # extract individual haplotypes#
for (k in 1:length(OTUs)){#
#
otu <- read.csv(paste(folder[i], "/", OTUs[k], "/", OTUs[k], "_tab.csv", sep=""), stringsAsFactors=F)#
#
# count sequences in OTU#
otu_abund <- sum(as.numeric(sub(".*size=(.*);", "\\1", otu$V1)))#
#
otu <- otu[otu$keep,] # keep high abund haplotypes#
otu <- cbind(otu[c(2, 11, 14)], "indiv"=as.numeric(sub(".*size=(.*);", "\\1", otu$V1)), otu_abund) # extract useful info#
#
exp <- rbind(exp, otu)#
}#
#
temp_name <- sub("_data/6_haplotypes/", "", folder[i])#
names(exp) <- c(paste(temp_name, "_OTU", sep=""), "V11", paste(temp_name, "_rel", sep=""), paste(temp_name, "_abund", sep=""), paste(temp_name, "_OTU_abund", sep=""))#
#
# merge exp haplotype tables from samples#
#
master_tab <- merge(master_tab, exp, by="V11", all=T)#
#
}#
#
# reorganise haplo table #
#
master_tab <- master_tab[c(1, (1:length(folder))*4-2, (1:length(folder))*4-1, (1:length(folder))*4,(1:length(folder))*4+1)]#
#
master_tab$V11 <- as.character(master_tab$V11)#
#
OTU <- NULL#
#
for (i in 1:length(folder)){#
keep <- as.vector(!is.na(master_tab[(1+i)]))#
OTU[keep] <- master_tab[keep, (1+i)]#
}#
#
master_tab <- cbind(OTU, master_tab[-c(2:(length(folder)+1))])#
master_tab <- master_tab[order(as.numeric(sub("OTU_", "", master_tab$OTU)), as.numeric(sub("Uniq", "", master_tab$V11)), decreasing=F),]#
names(master_tab)[2] <- "Haplotypes"#
#
# add sequences to table#
master_tab <- cbind(sort= c(1:nrow(master_tab)), master_tab, "sequ"=unlist(DNA_master[match(master_tab$Haplotypes, names(DNA_master))]))#
#
write.csv(master_tab, file="haplo_tab.csv", row.names=F)#
write.fasta(DNA_master[match(master_tab$Haplotypes, names(DNA_master))], names=paste(master_tab$OTU, master_tab$Haplotypes, sep="_"), "haplo_fasta.txt")#
temp <- "\nModule completed!"#
message(temp)#
cat(file="../log.txt", paste(Sys.time(), temp, "", sep="\n"), append=T, sep="\n")#
#
setwd("../")#
}
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
temp <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[25:30]
haplotyping(temp, 178)
list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[25:30]
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
# romania fwh1#
temp <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[11:16]#
haplotyping(temp, 178)
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[17:24]#
haplotyping(temp, 205)
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[17:24]
files
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[17:24]
files
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[17:24]#
haplotyping(temp, 205)
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
# Test Tiermix files for haplotyping! 170126#
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")#
#
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[1:8] #
haplotyping(files, 178)#
#
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[17:24]#
haplotyping(files, 205)#
# romania fwh1#
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[11:16]#
haplotyping(files, 178)#
#
# romania fwh2#
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[25:30]#
haplotyping(files, 205)
# 170426 jamp pipeline#
setwd("~/Documents/UNI_und_VORLESUNGEN/GitHub/JAMP/")#
#
#install.packages(c("bold", "XML", "seqinr"), dependencies=T)#
#install.packages("JAMP", repos = NULL, type="source")#
#install.packages("~/Desktop/bold-master", repos = NULL, type="source")#
library("JAMP") #v0.17#
library("bold")#
library("XML")#
# base directory#
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[11:16]
files
U_cluster_otus(files, filter=0.003)
files
file.rename("K_U_cluster_otus", "K_U_cluster_otus - fwh1")
files <- list.files("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP/F_U_max_ee/_data", full.names=T)[25:30]
U_cluster_otus(files, filter=0.003)
file.rename("L_U_cluster_otus", "L_U_cluster_otus - fwh2")
# 170426 jamp pipeline#
setwd("~/Documents/UNI_und_VORLESUNGEN/GitHub/JAMP/")#
#
#install.packages(c("bold", "XML", "seqinr"), dependencies=T)#
#install.packages("JAMP", repos = NULL, type="source")#
#install.packages("~/Desktop/bold-master", repos = NULL, type="source")#
library("JAMP") #v0.17#
library("bold")#
library("XML")#
# base directory#
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
Bold_web_hack("../170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
Bold_web_hack
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
Bold_web_hack("170427_bold_romania_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170427_bold_romania_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")#
#
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
MM=c(0.98, 0.95, 0.90, 0.85)
file="170118_bold_fwh1.txt"
folder="K_U_cluster_otus - fwh1"
file="170118_bold_fwh1.txt"#
folder="K_U_cluster_otus - fwh1"
oldwd <- getwd()#
setwd(folder)#
getwd()#
#cat(file="../log.txt", c("\n","Version v0.1", "\n"), append=T, sep="\n")#
#
dir.create("_BOLD_web_hack", showWarnings = FALSE)#
dir.create("_BOLD_web_hack/OTUs", showWarnings = FALSE)#
setwd("_BOLD_web_hack")
data <- readLines(file, warn=F)
OTU_start <- grep("Query: ", data)#
OTU_end <- which("Sampling Sites For Top Hits (>98% Match)"==data)#
#
OTU_end <- sort(c(which("Unable to match any records in the selected database. "==data), OTU_end))
OTU_end
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
file="170427_bold_romania_fwh1.txt"
oldwd <- getwd()#
setwd(folder)#
getwd()#
#cat(file="../log.txt", c("\n","Version v0.1", "\n"), append=T, sep="\n")#
#
dir.create("_BOLD_web_hack", showWarnings = FALSE)#
dir.create("_BOLD_web_hack/OTUs", showWarnings = FALSE)#
setwd("_BOLD_web_hack")#
#
data <- readLines(file, warn=F)#
OTU_start <- grep("Query: ", data)#
OTU_end <- which("Sampling Sites For Top Hits (>98% Match)"==data)#
#
OTU_end <- sort(c(which("Unable to match any records in the selected database. "==data), OTU_end))
OTU_end
i <- 1
temp <- data[OTU_start[i]:OTU_end[i]]#
#
OTU <- sub("Query: (.*) ", "\\1", data[OTU_start[i]])#
similarity <- as.numeric(temp)#
whois <- which(!is.na(similarity))#
whois_num <- whois#
#
Similarity <- similarity[whois]#
Status <- temp[whois+2]#
#
# get phylum#
#
whois <- c(which(temp =="Phylum\tClass\tOrder\tFamily\tGenus\tSpecies\tSimilarity (%)\tStatus")+1, whois[-length(whois)]+4)#
#
# Phylum
whois
temp <- data[OTU_start[i]:OTU_end[i]]
temp
sub("Query: (.*) ", "\\1", data[OTU_start[i]])
similarity <- as.numeric(temp)#
whois <- which(!is.na(similarity))#
whois_num <- whois
whois
whois_num
Similarity <- similarity[whois]#
Status <- temp[whois+2]
Status
temp
whois <- c(which(temp =="Phylum\tClass\tOrder\tFamily\tGenus\tSpecies\tSubspecies\tSimilarity (%)\tStatus")+1, whois[-length(whois)]+4)
whois
which(temp =="Phylum\tClass\tOrder\tFamily\tGenus\tSpecies\tSubspecies\tSimilarity (%)\tStatus")+1
Phylum <- temp[whois]
Phylum
Phylum <- temp[whois]#
Class <- temp[whois+2]#
Order <- temp[whois+4]#
Family <- temp[whois+6]#
Genus <- temp[whois+8]#
Species <- temp[whois+10]
Species
Subspecies <- temp[whois+12]
Subspecies
setwd("~/Documents/UNI_und_VORLESUNGEN/GitHub/JAMP/")
install.packages("JAMP", repos = NULL, type="source")
library("JAMP") #v0.17
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
# Bold web hack#
Bold_web_hack <- function(file=NA, folder="", MM=c(0.98, 0.95, 0.90, 0.85)){#
#
oldwd <- getwd()#
setwd(folder)#
getwd()#
#cat(file="../log.txt", c("\n","Version v0.1", "\n"), append=T, sep="\n")#
#
dir.create("_BOLD_web_hack", showWarnings = FALSE)#
dir.create("_BOLD_web_hack/OTUs", showWarnings = FALSE)#
setwd("_BOLD_web_hack")#
#
data <- readLines(file, warn=F)#
OTU_start <- grep("Query: ", data)#
OTU_end <- which("Sampling Sites For Top Hits (>98% Match)"==data)#
#
OTU_end <- sort(c(which("Unable to match any records in the selected database. "==data), OTU_end))#
for (i in 1:length(OTU_start)){#
temp <- data[OTU_start[i]:OTU_end[i]]#
#
OTU <- sub("Query: (.*) ", "\\1", data[OTU_start[i]])#
similarity <- as.numeric(temp)#
whois <- which(!is.na(similarity))#
whois_num <- whois#
#
Similarity <- similarity[whois]#
Status <- temp[whois+2]#
#
# get phylum#
#
whois <- c(which(temp =="Phylum\tClass\tOrder\tFamily\tGenus\tSpecies\tSubspecies\tSimilarity (%)\tStatus")+1, whois[-length(whois)]+4)#
#
# Phylum#
Phylum <- temp[whois]#
Class <- temp[whois+2]#
Order <- temp[whois+4]#
Family <- temp[whois+6]#
Genus <- temp[whois+8]#
Species <- temp[whois+10]#
Subspecies <- temp[whois+12] # ignore subspecies for now#
#
temp_tab <- data.frame(Phylum, Class, Order, Family, Genus, Species, Subspecies, Similarity, Status)#
#
write.csv(temp_tab, paste("OTUs/", OTU, ".csv", sep=""), row.names=F)#
}#
# subset taxonomy stuff#
#
files <- sub("Query: (.*) ", "\\1", data[OTU_start])#
#
BOLD_tab <- NULL#
#
i <- 5#
#
for (i in 1:length(files)){#
temp <- read.csv(paste("OTUs/", files[i], ".csv", sep=""), stringsAsFactors=F)#
#
if(nrow(temp)==0){ # no hits!#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], temp[1,]))#
#
} else {#
temp[temp==""] <- NA#
#
#remove taxa lower than treshhold!#
A <- temp$Similarity<MM[1]*100#
temp$Species[A] <- NA#
A <- temp$Similarity<MM[2]*100#
temp$Genus[A] <- NA#
A <- temp$Similarity<MM[3]*100#
temp$Family[A] <- NA#
A <- temp$Similarity<MM[4]*100#
temp$Order[A] <- NA#
#
# get taxon#
END <- T # stop if written in table#
who_sp <- which(temp$Similarity>= MM[1]*100)#
who_sp <- which(!is.na(temp$Species[who_sp])) #rm NA#
#
if(length(who_sp)>0){ # if species present#
tab_species <- as.data.frame(table(temp$Species[who_sp]), stringsAsFactors=F)#
tab_species <- tab_species[order(tab_species$Freq, decreasing=T),]#
tab_species <- if(is.vector(tab_species)){tab_species[1]} else{tab_species[1,1]}#
#
exp <- temp[who_sp[temp$Species[who_sp]==tab_species][1],]#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
who_gen <- which(temp$Similarity>= MM[2]*100)#
who_gen <- which(!is.na(temp$Genus[who_gen])) #rm NA#
#
if(length(who_gen)>0 & END){#
#
tab_genus <- as.data.frame(table(temp$Genus[who_gen]), stringsAsFactors=F)#
tab_genus <- tab_genus[order(tab_genus$Freq, decreasing=T),]#
tab_genus <- if(is.vector(tab_genus)){tab_genus[1]} else{tab_genus[1,1]}#
#
exp <- temp[who_gen[temp$Genus[who_gen]==tab_genus][1],]#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
#
}#
#
who_fam <- which(temp$Similarity>= MM[3]*100)#
who_fam <- which(!is.na(temp$Family[who_fam])) #rm NA#
#
if(length(who_fam)>0 & END){#
#
tab_family <- as.data.frame(table(temp$Family[who_fam]), stringsAsFactors=F)#
tab_family <- tab_family[order(tab_family $Freq, decreasing=T),]#
tab_family <- if(is.vector(tab_family)){tab_family[1]} else{tab_family[1,1]}#
exp <- temp[who_fam[temp$Family[who_fam]==tab_family][1],]#
exp[5:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
who_ord <- which(temp$Similarity>= MM[4]*100)#
who_ord <- which(!is.na(temp$Order[who_ord])) #rm NA#
#
if(length(who_fam)>0 & END){#
tab_order <- as.data.frame(table(temp$Order[who_ord]), stringsAsFactors=F)#
tab_order <- tab_order[order(tab_order$Freq, decreasing=T),]#
tab_order <- if(is.vector(tab_order)){tab_order[1]} else{tab_order[1,1]}#
#
exp <- temp[who_ord[temp$Order[who_ord]==tab_order][1],]#
exp[4:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
if(END){#
exp <- temp[1,]#
exp[3:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], temp[1,]))#
}#
} # "skipp no hits"#
} # taxa tab loop end#
BOLD_tab2 <- data.frame(BOLD_tab, stringsAsFactors=F)#
names(BOLD_tab2)[1] <- "OTU_ID"#
write.csv(BOLD_tab2, file=sub(".txt", "_taxonomy.csv", file), row.names=F)#
#
setwd(oldwd)#
}
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
# Bold web hack#
Bold_web_hack <- function(file=NA, folder="", MM=c(0.98, 0.95, 0.90, 0.85)){#
#
oldwd <- getwd()#
setwd(folder)#
getwd()#
#cat(file="../log.txt", c("\n","Version v0.1", "\n"), append=T, sep="\n")#
#
dir.create("_BOLD_web_hack", showWarnings = FALSE)#
dir.create("_BOLD_web_hack/OTUs", showWarnings = FALSE)#
setwd("_BOLD_web_hack")#
#
data <- readLines(file, warn=F)#
OTU_start <- grep("Query: ", data)#
OTU_end <- which("Sampling Sites For Top Hits (>98% Match)"==data)#
#
OTU_end <- sort(c(which("Unable to match any records in the selected database. "==data), OTU_end))#
for (i in 1:length(OTU_start)){#
temp <- data[OTU_start[i]:OTU_end[i]]#
#
OTU <- sub("Query: (.*) ", "\\1", data[OTU_start[i]])#
similarity <- as.numeric(temp)#
whois <- which(!is.na(similarity))#
whois_num <- whois#
#
Similarity <- similarity[whois]#
Status <- temp[whois+2]#
#
# get phylum#
#
whois <- c(which(temp =="Phylum\tClass\tOrder\tFamily\tGenus\tSpecies\tSubspecies\tSimilarity (%)\tStatus")+1, whois[-length(whois)]+4)#
#
# Phylum#
Phylum <- temp[whois]#
Class <- temp[whois+2]#
Order <- temp[whois+4]#
Family <- temp[whois+6]#
Genus <- temp[whois+8]#
Species <- temp[whois+10]#
Subspecies <- temp[whois+12] # ignore subspecies for now#
#
temp_tab <- data.frame(Phylum, Class, Order, Family, Genus, Species, Similarity, Status)#
#
write.csv(temp_tab, paste("OTUs/", OTU, ".csv", sep=""), row.names=F)#
}#
# subset taxonomy stuff#
#
files <- sub("Query: (.*) ", "\\1", data[OTU_start])#
#
BOLD_tab <- NULL#
#
i <- 5#
#
for (i in 1:length(files)){#
temp <- read.csv(paste("OTUs/", files[i], ".csv", sep=""), stringsAsFactors=F)#
#
if(nrow(temp)==0){ # no hits!#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], temp[1,]))#
#
} else {#
temp[temp==""] <- NA#
#
#remove taxa lower than treshhold!#
A <- temp$Similarity<MM[1]*100#
temp$Species[A] <- NA#
A <- temp$Similarity<MM[2]*100#
temp$Genus[A] <- NA#
A <- temp$Similarity<MM[3]*100#
temp$Family[A] <- NA#
A <- temp$Similarity<MM[4]*100#
temp$Order[A] <- NA#
#
# get taxon#
END <- T # stop if written in table#
who_sp <- which(temp$Similarity>= MM[1]*100)#
who_sp <- which(!is.na(temp$Species[who_sp])) #rm NA#
#
if(length(who_sp)>0){ # if species present#
tab_species <- as.data.frame(table(temp$Species[who_sp]), stringsAsFactors=F)#
tab_species <- tab_species[order(tab_species$Freq, decreasing=T),]#
tab_species <- if(is.vector(tab_species)){tab_species[1]} else{tab_species[1,1]}#
#
exp <- temp[who_sp[temp$Species[who_sp]==tab_species][1],]#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
who_gen <- which(temp$Similarity>= MM[2]*100)#
who_gen <- which(!is.na(temp$Genus[who_gen])) #rm NA#
#
if(length(who_gen)>0 & END){#
#
tab_genus <- as.data.frame(table(temp$Genus[who_gen]), stringsAsFactors=F)#
tab_genus <- tab_genus[order(tab_genus$Freq, decreasing=T),]#
tab_genus <- if(is.vector(tab_genus)){tab_genus[1]} else{tab_genus[1,1]}#
#
exp <- temp[who_gen[temp$Genus[who_gen]==tab_genus][1],]#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
#
}#
#
who_fam <- which(temp$Similarity>= MM[3]*100)#
who_fam <- which(!is.na(temp$Family[who_fam])) #rm NA#
#
if(length(who_fam)>0 & END){#
#
tab_family <- as.data.frame(table(temp$Family[who_fam]), stringsAsFactors=F)#
tab_family <- tab_family[order(tab_family $Freq, decreasing=T),]#
tab_family <- if(is.vector(tab_family)){tab_family[1]} else{tab_family[1,1]}#
exp <- temp[who_fam[temp$Family[who_fam]==tab_family][1],]#
exp[5:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
who_ord <- which(temp$Similarity>= MM[4]*100)#
who_ord <- which(!is.na(temp$Order[who_ord])) #rm NA#
#
if(length(who_fam)>0 & END){#
tab_order <- as.data.frame(table(temp$Order[who_ord]), stringsAsFactors=F)#
tab_order <- tab_order[order(tab_order$Freq, decreasing=T),]#
tab_order <- if(is.vector(tab_order)){tab_order[1]} else{tab_order[1,1]}#
#
exp <- temp[who_ord[temp$Order[who_ord]==tab_order][1],]#
exp[4:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
if(END){#
exp <- temp[1,]#
exp[3:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], temp[1,]))#
}#
} # "skipp no hits"#
} # taxa tab loop end#
BOLD_tab2 <- data.frame(BOLD_tab, stringsAsFactors=F)#
names(BOLD_tab2)[1] <- "OTU_ID"#
write.csv(BOLD_tab2, file=sub(".txt", "_taxonomy.csv", file), row.names=F)#
#
setwd(oldwd)#
}
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
setwd("~/Documents/UNI_und_VORLESUNGEN/GitHub/JAMP/")
install.packages("JAMP", repos = NULL, type="source")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170118_bold_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170427_bold_romania_fwh1.txt", "K_U_cluster_otus - fwh1")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
file="170427_bold_romania_fwh1.txt"#
folder="K_U_cluster_otus - fwh1"
oldwd <- getwd()#
setwd(folder)#
getwd()#
#cat(file="../log.txt", c("\n","Version v0.1", "\n"), append=T, sep="\n")#
#
dir.create("_BOLD_web_hack", showWarnings = FALSE)#
dir.create("_BOLD_web_hack/OTUs", showWarnings = FALSE)#
setwd("_BOLD_web_hack")#
#
data <- readLines(file, warn=F)#
OTU_start <- grep("Query: ", data)#
OTU_end <- which("Sampling Sites For Top Hits (>98% Match)"==data)#
#
OTU_end <- sort(c(which("Unable to match any records in the selected database. "==data), OTU_end))
i <- 6
temp <- data[OTU_start[i]:OTU_end[i]]
temp
temp <- data[OTU_start[i]:OTU_end[i]]#
#
OTU <- sub("Query: (.*) ", "\\1", data[OTU_start[i]])#
similarity <- as.numeric(temp)#
whois <- which(!is.na(similarity))#
whois_num <- whois#
#
Similarity <- similarity[whois]#
Status <- temp[whois+2]#
#
# get phylum#
#
whois <- c(which(temp =="Phylum\tClass\tOrder\tFamily\tGenus\tSpecies\tSubspecies\tSimilarity (%)\tStatus")+1, whois[-length(whois)]+4)#
#
# Phylum#
Phylum <- temp[whois]#
Class <- temp[whois+2]#
Order <- temp[whois+4]#
Family <- temp[whois+6]#
Genus <- temp[whois+8]#
Species <- temp[whois+10]#
Subspecies <- temp[whois+12] # ignore subspecies for now#
#
temp_tab <- data.frame(Phylum, Class, Order, Family, Genus, Species, Similarity, Status)
temp_tab
whois[-length(whois)]+4
length(whois[-length(whois)]+4)
temp <- data[OTU_start[i]:OTU_end[i]]
OTU <- sub("Query: (.*) ", "\\1", data[OTU_start[i]])
similarity <- as.numeric(temp)
similarity
whois <- which(!is.na(similarity))
whois
length(whois)
whois_num <- whois
whois_num
similarity[whois]
temp[whois+2]
temp_tab <- data.frame(Phylum, Class, Order, Family, Genus, Species, Similarity, Status)
temp_tab
nrow(temp_tab)
k
k <- 9
temp_tab[k, 1:6]
is.numeric(temp_tab[k, 1:6])
temp_tab[k, 1:6]
is.numeric(unlist(temp_tab[k, 1:6]))
is.numeric(c(temp_tab[k, 1:6]))
c(temp_tab[k, 1:6])
is.numeric(as.vector(temp_tab[k, 1:6]))
as.vector(temp_tab[k, 1:6])
is.numeric(as.vector(c(temp_tab[k, 1:6])))
as.vector(c(temp_tab[k, 1:6]))
as.vector(unlist(temp_tab[k, 1:6]))
is.numeric(as.vector(unlist(temp_tab[k, 1:6])))
as.numeric(as.vector(unlist(temp_tab[k, 1:6])))
is.numeric(as.numeric(as.vector(unlist(temp_tab[k, 1:6]))))
as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0
which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0)
which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0, warn=F)
which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0, warnings=F)
which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0)
kill <- which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0)
is.numeric(kill)
temp_tab[k, kill:6]
temp_tab[k, kill:6] <- ""
temp_tab
temp_tab[k, kill:6] <- "a"
temp_tab[k, kill:6] <- "aads"
temp_tab[k, kill:6]
k <- 8
temp_tab[k, kill:6]
temp_tab[k, kill:6] <- "aads"
temp_tab[k, 6:6]
temp_tab <- data.frame(Phylum, Class, Order, Family, Genus, Species, Similarity, Status, stringsAsFactors=F)
temp_tab
for (k in 1:nrow(temp_tab)){#
kill <- which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0)[1]#
if(is.numeric(kill)){#
temp_tab[k, kill:6] <- "aads"#
#
}#
}
temp_tab
temp_tab[k, kill:6] <- "aads"
temp_tab
k <- 9
kill <- which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0)[1]
kill
is.numeric(kill)
temp_tab[k, kill:6]
temp_tab[k, kill:6] <- ""
temp_tab
which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0)
for (k in 1:nrow(temp_tab)){#
kill <- which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0)[1]#
if(is.numeric(kill)){#
temp_tab[k, kill:6] <- ""#
#
}#
}
kill
for (k in 1:nrow(temp_tab)){#
kill <- which(as.numeric(as.vector(unlist(temp_tab[k, 1:6])))>0)[1]#
if(!is.na(kill)){#
temp_tab[k, kill:6] <- ""#
#
}#
}
temp_tab
?as.numeric
kill <- which(suppressWarnings(as.numeric(as.vector(unlist(temp_tab[k, 1:6]))))>0)[1]
temp_tab <- data.frame(Phylum, Class, Order, Family, Genus, Species, Similarity, Status, stringsAsFactors=F)#
#
for (k in 1:nrow(temp_tab)){#
kill <- which(suppressWarnings(as.numeric(as.vector(unlist(temp_tab[k, 1:6]))))>0)[1]#
if(!is.na(kill)){#
temp_tab[k, kill:6] <- ""#
#
}#
}
temp_tab
# Bold web hack#
Bold_web_hack <- function(file=NA, folder="", MM=c(0.98, 0.95, 0.90, 0.85)){#
#
oldwd <- getwd()#
setwd(folder)#
getwd()#
#cat(file="../log.txt", c("\n","Version v0.1", "\n"), append=T, sep="\n")#
#
dir.create("_BOLD_web_hack", showWarnings = FALSE)#
dir.create("_BOLD_web_hack/OTUs", showWarnings = FALSE)#
setwd("_BOLD_web_hack")#
#
data <- readLines(file, warn=F)#
OTU_start <- grep("Query: ", data)#
OTU_end <- which("Sampling Sites For Top Hits (>98% Match)"==data)#
#
OTU_end <- sort(c(which("Unable to match any records in the selected database. "==data), OTU_end))#
#
i <- 6#
for (i in 1:length(OTU_start)){#
temp <- data[OTU_start[i]:OTU_end[i]]#
#
OTU <- sub("Query: (.*) ", "\\1", data[OTU_start[i]])#
similarity <- as.numeric(temp)#
whois <- which(!is.na(similarity))#
whois_num <- whois#
#
Similarity <- similarity[whois]#
Status <- temp[whois+2]#
#
# get phylum#
#
whois <- c(which(temp =="Phylum\tClass\tOrder\tFamily\tGenus\tSpecies\tSubspecies\tSimilarity (%)\tStatus")+1, whois[-length(whois)]+4)#
# Phylum#
Phylum <- temp[whois]#
Class <- temp[whois+2]#
Order <- temp[whois+4]#
Family <- temp[whois+6]#
Genus <- temp[whois+8]#
Species <- temp[whois+10]#
Subspecies <- temp[whois+12] # ignore subspecies for now#
#
temp_tab <- data.frame(Phylum, Class, Order, Family, Genus, Species, Similarity, Status, stringsAsFactors=F)#
#
for (k in 1:nrow(temp_tab)){#
kill <- which(suppressWarnings(as.numeric(as.vector(unlist(temp_tab[k, 1:6]))))>0)[1]#
if(!is.na(kill)){#
temp_tab[k, kill:6] <- ""#
#
}#
}#
#
write.csv(temp_tab, paste("OTUs/", OTU, ".csv", sep=""), row.names=F)#
}#
# subset taxonomy stuff#
#
files <- sub("Query: (.*) ", "\\1", data[OTU_start])#
#
BOLD_tab <- NULL#
#
i <- 5#
#
for (i in 1:length(files)){#
temp <- read.csv(paste("OTUs/", files[i], ".csv", sep=""), stringsAsFactors=F)#
#
if(nrow(temp)==0){ # no hits!#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], temp[1,]))#
#
} else {#
temp[temp==""] <- NA#
#
#remove taxa lower than treshhold!#
A <- temp$Similarity<MM[1]*100#
temp$Species[A] <- NA#
A <- temp$Similarity<MM[2]*100#
temp$Genus[A] <- NA#
A <- temp$Similarity<MM[3]*100#
temp$Family[A] <- NA#
A <- temp$Similarity<MM[4]*100#
temp$Order[A] <- NA#
#
# get taxon#
END <- T # stop if written in table#
who_sp <- which(temp$Similarity>= MM[1]*100)#
who_sp <- which(!is.na(temp$Species[who_sp])) #rm NA#
#
if(length(who_sp)>0){ # if species present#
tab_species <- as.data.frame(table(temp$Species[who_sp]), stringsAsFactors=F)#
tab_species <- tab_species[order(tab_species$Freq, decreasing=T),]#
tab_species <- if(is.vector(tab_species)){tab_species[1]} else{tab_species[1,1]}#
#
exp <- temp[who_sp[temp$Species[who_sp]==tab_species][1],]#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
who_gen <- which(temp$Similarity>= MM[2]*100)#
who_gen <- which(!is.na(temp$Genus[who_gen])) #rm NA#
#
if(length(who_gen)>0 & END){#
#
tab_genus <- as.data.frame(table(temp$Genus[who_gen]), stringsAsFactors=F)#
tab_genus <- tab_genus[order(tab_genus$Freq, decreasing=T),]#
tab_genus <- if(is.vector(tab_genus)){tab_genus[1]} else{tab_genus[1,1]}#
#
exp <- temp[who_gen[temp$Genus[who_gen]==tab_genus][1],]#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
#
}#
#
who_fam <- which(temp$Similarity>= MM[3]*100)#
who_fam <- which(!is.na(temp$Family[who_fam])) #rm NA#
#
if(length(who_fam)>0 & END){#
#
tab_family <- as.data.frame(table(temp$Family[who_fam]), stringsAsFactors=F)#
tab_family <- tab_family[order(tab_family $Freq, decreasing=T),]#
tab_family <- if(is.vector(tab_family)){tab_family[1]} else{tab_family[1,1]}#
exp <- temp[who_fam[temp$Family[who_fam]==tab_family][1],]#
exp[5:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
who_ord <- which(temp$Similarity>= MM[4]*100)#
who_ord <- which(!is.na(temp$Order[who_ord])) #rm NA#
#
if(length(who_fam)>0 & END){#
tab_order <- as.data.frame(table(temp$Order[who_ord]), stringsAsFactors=F)#
tab_order <- tab_order[order(tab_order$Freq, decreasing=T),]#
tab_order <- if(is.vector(tab_order)){tab_order[1]} else{tab_order[1,1]}#
#
exp <- temp[who_ord[temp$Order[who_ord]==tab_order][1],]#
exp[4:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], exp))#
END <- F#
}#
#
if(END){#
exp <- temp[1,]#
exp[3:6] <- NA#
BOLD_tab <- rbind(BOLD_tab, cbind(files[i], temp[1,]))#
}#
} # "skipp no hits"#
} # taxa tab loop end#
BOLD_tab2 <- data.frame(BOLD_tab, stringsAsFactors=F)#
names(BOLD_tab2)[1] <- "OTU_ID"#
write.csv(BOLD_tab2, file=sub(".txt", "_taxonomy.csv", file), row.names=F)#
#
setwd(oldwd)#
}
setwd("~/Documents/UNI_und_VORLESUNGEN/11 phd projects/1 Meta HAPLOTYPING/1 JAMP")
Bold_web_hack("170427_bold_romania_fwh1.txt", "K_U_cluster_otus - fwh1")
Bold_web_hack("170427_bold_romania_fwh2.txt", "L_U_cluster_otus - fwh2")
warnings
warnings()
setwd("~/Documents/UNI_und_VORLESUNGEN/GitHub/JAMP/")
install.packages("JAMP", repos = NULL, type="source")
